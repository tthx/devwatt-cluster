<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>
  <!--
  	Configurations for ResourceManager
  -->
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>master</value>
  </property>
  <property>
    <name>yarn.resourcemanager.nodes.exclude-path</name>
    <value>/etc/hadoop/exclude</value>
  </property>
  <property>
    <name>yarn.webapp.ui2.enable</name>
    <value>true</value>
  </property>
  <!--
    Capacity Scheduler
  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
    <description>The class to use as the resource scheduler. Default:  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</description>
  </property>
  -->
  <!--
    Fair Scheduler
  -->
  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
  </property>
  <property>
    <name>yarn.scheduler.fair.allocation.file</name>
    <value>fair-scheduler.xml</value>
    <description>Path to allocation file. An allocation file is an XML manifest describing queues and their properties, in addition to certain policy defaults. This file must be in the XML format described in the next section. If a relative path is given, the file is searched for on the classpath (which typically includes the Hadoop conf directory). Defaults to fair-scheduler.xml.</description>
  </property>
  <property>
    <name>yarn.scheduler.fair.preemption</name>
    <value>true</value>
    <description>Whether to use preemption. Defaults to false.</description>
  </property>
  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>1024</value>
    <description>The minimum allocation for every container request at the RM in MBs. Memory requests lower than this will be set to the value of this property. Additionally, a node manager that is configured to have less memory than this value will be shut down by the resource manager. Default: 1024</description>
  </property>
  <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>8192</value>
    <description>The maximum allocation for every container request at the RM in MBs. Memory requests higher than this will throw an InvalidResourceRequestException. Default: 8192</description>
  </property>
  <property>
    <name>yarn.scheduler.minimum-allocation-vcores</name>
    <value>1</value>
    <description>The minimum allocation for every container request at the RM in terms of virtual CPU cores. Requests lower than this will be set to the value of this property. Additionally, a node manager that is configured to have fewer virtual cores than this value will be shut down by the resource manager. Default: 1</description>
  </property>
  <property>
    <name>yarn.scheduler.maximum-allocation-vcores</name>
    <value>4</value>
    <description>The maximum allocation for every container request at the RM in terms of virtual CPU cores. Requests higher than this will throw an InvalidResourceRequestException. Default: 4</description>
  </property>
  <!--
  	Configurations for NodeManager
  -->
  <property>
    <name>yarn.nodemanager.resource.detect-hardware-capabilities</name>
    <value>true</value>
    <description>Enable auto-detection of node capabilities such as memory and CPU. Default: false</description>
  </property>
  <property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>-1</value>
    <description>Number of vcores that can be allocated for containers. This is used by the RM scheduler when allocating resources for containers. This is not used to limit the number of CPUs used by YARN containers. If it is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically determined from the hardware in case of Windows and Linux. In other cases, number of vcores is 8 by default. Default: -1</description>
  </property>
  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>-1</value>
    <description>Amount of physical memory, in MB, that can be allocated for containers. If set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically calculated(in case of Windows and Linux). In other cases, the default is 8192MB. Default: -1</description>
  </property>
  <property>
    <name>yarn.nodemanager.vmem-pmem-ratio</name>
    <value>2.1</value>
    <description>Ratio between virtual memory to physical memory when setting memory limits for containers. Container allocations are expressed in terms of physical memory, and virtual memory usage is allowed to exceed this allocation by this ratio. Default: -1</description>
  </property>
  <property>
    <name>yarn.nodemanager.local-dirs</name>
    <value>/var/yarn/local</value>
  </property>
  <property>
    <name>yarn.nodemanager.log-dirs</name>
    <value>/var/yarn/log</value>
  </property>
  <property>
    <name>yarn.nodemanager.log.retain-seconds</name>
    <value>10800</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.env-whitelist</name>
    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
  </property>
  <!--
    Log aggregation
  -->
  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
    <description>Whether to enable log aggregation. Log aggregation collects each container's logs and moves these logs onto a file-system, for e.g. HDFS, after the application completes. Users can configure the "yarn.nodemanager.remote-app-log-dir" and "yarn.nodemanager.remote-app-log-dir-suffix" properties to determine where these logs are moved to. Users can access the logs via the Application Timeline Server. Default: false</description>
  </property>
  <property>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value>/home/yarn/log</value>
    <description>Where to aggregate logs to. Default: /tmp/logs</description>
  </property>
  <property>
    <name>yarn.nodemanager.remote-app-log-dir-suffix</name>
    <value>logs</value>
    <description>The remote log dir will be created at {yarn.nodemanager.remote-app-log-dir}/${user}/{thisParam}. Default: logs</description>
  </property>
  <property>
    <name>yarn.log.server.url</name>
    <value>master:10020</value>
    <description>URL for log aggregation server</description>
  </property>
  <property>
    <name>yarn.timeline-service.hostname</name>
    <value>master</value>
    <description>The hostname of the Timeline service web application. Defaults to 0.0.0.0</description>
  </property>
  <property>
    <name>yarn.log.server.web-service.url</name>
    <value>master:8188/ws/v2/applicationlog</value>
    <description>TimelineService v.2 supports serving aggregated logs of historical apps. To enable this, configure “yarn.log.server.web-service.url” to “${yarn .timeline-service.hostname}:8188/ws/v2/applicationlog”</description>
  </property>
  <!--
    Secure Container
  -->
  <property>
    <name>yarn.nodemanager.container-executor.class</name>
    <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users</name>
    <value>false</value>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.group</name>
    <value>hadoop</value>
  </property>
  <!--
  	Configurations for History Server
  -->
  <property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>3600</value>
  </property>
  <property>
    <name>yarn.log-aggregation.retain-check-interval-seconds</name>
    <value>1800</value>
  </property>
  <!--
    Timeline service v2
  -->
  <property>
    <name>yarn.timeline-service.enabled</name>
    <value>true</value>
    <description>Indicate to clients whether Timeline service is enabled or not. If enabled, the TimelineClient library used by applications will post entities and events to the Timeline server. Defaults to false.</description>
  </property>
  <property>
    <name>yarn.timeline-service.version</name>
    <value>2.0f</value>
    <description>Indicate what is the current version of the running timeline service. For example, if “yarn.timeline-service.version” is 1.5, and “yarn.timeline-service.enabled” is true, it means the cluster will and must bring up the timeline service v.1.5 (and nothing else). On the client side, if the client uses the same version of timeline service, it must succeed. If the client chooses to use a smaller version in spite of this, then depending on how robust the compatibility story is between versions, the results may vary. Defaults to 1.0f.</description>
  </property>
  <property>
    <name>yarn.system-metrics-publisher.enabled</name>
    <value>true</value>
    <description>The setting that controls whether yarn system metrics is published on the Timeline service or not by RM And NM. Defaults to false.</description>
  </property>
  <property>
    <name>yarn.timeline-service.schema.prefix</name>
    <value>prod</value>
    <description>The schema prefix for hbase tables. Defaults to “prod.”.</description>
  </property>
  <property>
    <name>yarn.timeline-service.hostname</name>
    <value>master</value>
    <description>The hostname of the Timeline service web application. Defaults to 0.0.0.0</description>
  </property>
  <property>
    <name>yarn.timeline-service.hbase.configuration.file</name>
    <value>file:/etc/hbase/hbase-site.xml</value>
    <description>Optional URL to an hbase-site.xml configuration file to be
    used to connect to the timeline-service hbase cluster. If empty or not
    specified, then the HBase configuration will be loaded from the classpath.
    When specified the values in the specified configuration file will override
    those from the ones that are present on the classpath.
    </description>
  </property>
  <property>
    <name>yarn.timeline-service.hbase.coprocessor.jar.hdfs.location</name>
    <value>/home/hbase/coprocessor/hadoop-yarn-server-timelineservice.jar</value>
    <description>The default hdfs location for flowrun coprocessor jar.</description>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle,timeline_collector</value>
    <description>A comma separated list of services where service name should only contain a-zA-Z0-9_ and can not start with numbers</description>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.timeline_collector.class</name>
    <value>org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService</value>
  </property>
  <property>
    <name>yarn.rm.system-metrics-publisher.emit-container-events</name>
    <value>true</value>
    <description>The setting that controls whether yarn container events are
    published to the timeline service or not by RM. This configuration setting
    is for ATS V2.</description>
  </property>
  <property>
    <name>yarn.resourcemanager.cluster-id</name>
    <value>tth_cluster</value>
  </property>
</configuration>
