<configuration>

  <!-- Metastore -->
  <property>
    <name>metastore.thrift.uris</name>
    <value>thrift://master:9083</value>
    <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore. Format:
    thrift://#HOST#:#PORT#[, thrift://#HOST#:#PORT#...]
    HOST = hostname, PORT = should be set to match metastore.thrift.port on the server (which defaults to 9083. You can provide multiple servers in a comma separate list.
    Default: none</description>
  </property>

  <property>
    <name>metastore.thrift.port</name>
    <value>9083</value>
    <description>Port Thrift will listen on. Default: 9083</description>
  </property>

  <property>
    <name>hive.metastore.warehouse.dir</name>
    <value>hdfs://master:9000/home/hive/warehouse</value>
    <description>URI of the default location for tables in the default catalog and database. Default: /user/hive/warehouse</description>
  </property>

  <property>
    <name>datanucleus.schema.autoCreateAll</name>
    <value>false</value>
    <description>Auto creates the necessary schema in the RDBMS at startup if one does not exist. Set this to false after creating it once. To enable auto create also set hive.metastore.schema.verification=false. Auto creation is not recommended in production; run schematool instead. Default: false</description>
  </property>

  <!--
  <property>
    <name>datanucleus.schema.autoCreateTables</name>
    <value>true</value>
  </property>

  <property>
    <name>datanucleus.fixedDatastore</name>
    <value>true</value>
  </property>

  <property>
    <name>datanucleus.autoStartMechanism</name> 
    <value>SchemaTable</value>
  </property>
  -->

  <property>
    <name>metastore.schema.verification</name>
    <value>true</value>
    <description>Enforce metastore schema version consistency. When set to true: verify that version information stored in the RDBMS is compatible with the version of the Metastore jar. Also disable automatic schema migration. Users are required to manually migrate the schema after upgrade, which ensures proper schema migration. This setting is strongly recommended in production.
    When set to false: warn if the version information stored in RDBMS doesn't match the version of the Metastore jar and allow auto schema migration. Default: true</description>
  </property>

  <!-- Useless...
  <property>
    <name>metastore.log4j.file</name>
    <value>/etc/metastore/metastore-log4j2.properties</value>
    <description>Log4j configuration file. If unset will look for metastore-log4j2.properties in $METASTORE_HOME/conf. Default: none</description>
  </property>
  -->

  <property>
    <name>metastore.stats.autogather</name>
    <value>true</value>
    <description>Whether to automatically gather basic statistics during insert commands. Default: true</description>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:postgresql://master/metastore</value>
    <description>JDBC connect string for a JDBC metastore. Default: jdbc:derby:;databaseName=metastore_db;create=true</description>
  </property>
   
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>org.postgresql.Driver</value>
    <description>Driver class name for a JDBC metastore. Default: org.apache.derby.jdbc.EmbeddedDriver</description>
  </property>
   
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
    <description>Username to use against metastore database. Default: APP</description>
  </property>
   
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>azerty</value>
    <description>Password to use against metastore database. Default: mine</description>
  </property>
   
  <property>
    <name>metastore.try.direct.sql.ddl</name>
    <value>false</value>
    <description>Same as Configuration Properties#hive.metastore.try.direct.sql, for read statements within a transaction that modifies metastore data. Due to non-standard behavior in Postgres, if a direct SQL select query has incorrect syntax or something similar inside a transaction, the entire transaction will fail and fall-back to DataNucleus will not be possible. You should disable the usage of direct SQL inside transactions if that happens in your case.

    This can be configured on a per client basis by using the "set metaconf:hive.metastore.try.direct.sql.ddl=#value#" command, starting with Hive 0.14.0 (HIVE-7532). Default: true</description>
  </property>

  <property>
    <name>metastore.rawstore.impl</name>
    <value>org.apache.hadoop.hive.metastore.cache.CachedStore</value>
    <description>Name of the class that implements org.apache.hadoop.hive.metastore.rawstore interface. This class is used to store and retrieval of raw metadata objects such as table, database.

    As of Hive 3.0 there are two implementations. The default implementation (ObjectStore) queries the database directly. HIVE-16520 introduced a new CachedStore (full class name is org.apache.hadoop.hive.metastore.cache.CachedStore) that caches retrieved objects in memory on the Metastore. Default: org.apache.hadoop.hive.metastore.ObjectStore</description>
  </property>

  <!-- Metastore metrics -->
  <property>
    <name>hive.metastore.metrics.enabled</name>
    <value>true</value>
    <description>Enable metrics on the Hive Metastore Service. Default: false</description>
  </property>

  <!-- Storage Based Authorization in the Metastore Server -->
  <!--
  <property>
    <name>hive.metastore.pre.event.listeners</name>
    <value>org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener</value>
    <description>The pre-event listener classes to be loaded on the metastore side to run code whenever databases, tables, and partitions are created, altered, or dropped. Set this configuration property to org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener in hive-site.xml to turn on Hive metastore-side security. Default: empty</description>
  </property>

  <property>
    <name>hive.security.metastore.authorization.manager</name>
    <value>org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider</value>
    <description>Hive 0.13 and earlier: The authorization manager class name to be used in the metastore for authorization. The user-defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider.

    Hive 0.14 and later: Names of authorization manager classes (comma separated) to be used in the metastore for authorization. User-defined authorization classes should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider. All authorization manager classes have to successfully authorize the metastore API call for the command execution to be allowed.

    The DefaultHiveMetastoreAuthorizationProvider implements the standard Hive grant/revoke model. A storage-based authorization implementation is also provided to use as the value of this configuration property:

    org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider

    which uses HDFS permissions to provide authorization instead of using Hive-style grant-based authorization. Default: org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider</description>
  </property>
  -->
  
  <!-- SQL Standard Based Hive Authorization -->
  <!--
  <property>
    <name>hive.users.in.admin.role</name>
    <value>hive,impala</value>
    <description>A comma separated list of users which will be added to the ADMIN role when the metastore starts up. More users can still be added later on. Default: empty</description>
  </property>
  -->

  <!-- HiveServer2 -->
  <property>
    <name>hive.server2.thrift.port</name>
    <value>10000</value>
    <description>Port number of HiveServer2 Thrift interface. Can be overridden by setting $HIVE_SERVER2_THRIFT_PORT. Default: 10000.</description>
  </property>

  <property>
    <name>hive.server2.thrift.http.port</name>
    <value>10001</value>
    <description>Port number when in HTTP mode. Default: 10001.</description>
  </property>

  <property>
    <name>hive.server2.thrift.bind.host</name>
    <value>master</value>
    <description>Bind host on which to run the HiveServer2 Thrift interface. Can be overridden by setting $HIVE_SERVER2_THRIFT_BIND_HOST. Default: localhost.</description>
  </property>

  <property>
    <name>hive.server2.transport.mode</name>
    <value>binary</value>
    <description>Server transport mode. Value can be "binary" or "http". Default: binary.</description>
  </property>

  <!-- Queues -->
  <property>
    <name>hive.server2.tez.default.queues</name>
    <value>hive</value>
    <description>A list of comma separated values corresponding to YARN queues of the same name. When HiveServer2 is launched in Tez mode, this configuration needs to be set for multiple Tez sessions to run in parallel on the cluster. Default: (empty)</description>
  </property>

  <property>
    <name>hive.server2.tez.initialize.default.sessions</name>
    <value>true</value>
    <description>This flag is used in HiveServer 2 to enable a user to use HiveServer 2 without turning on Tez for HiveServer 2. The user could potentially want to run queries over Tez without the pool of sessions. Default: false</description>
  </property>

  <property>
    <name>hive.server2.tez.sessions.per.default.queue</name>
    <value>1</value>
    <description>A positive integer that determines the number of Tez sessions that should be launched on each of the queues specified by hive.server2.tez.default.queues. Determines the parallelism on each queue.

    Number of Concurrent Queries LLAP support. This will result in spawning equal number of TEZ AM. 

    Default: 1.</description>
  </property>

  <!-- Logging -->
  <property>
    <name>hive.server2.logging.operation.enabled</name>
    <value>true</value>
    <description>When true, HiveServer2 will save operation logs and make them available for clients. Default: true.</description>
  </property>

  <property>
    <name>hive.server2.logging.operation.log.location</name>
    <value>/var/hive/tmp/${user.name}/operation_logs</value>
    <description>Top level directory where operation logs are stored if logging functionality is enabled. Default: ${java.io.tmpdir}/${user.name}/operation_logs.</description>
  </property>

  <property>
    <name>hive.server2.logging.operation.level</name>
    <value>EXECUTION</value>
    <description>HiveServer2 operation logging mode available to clients to be set at session level.

    For this to work, hive.server2.logging.operation.enabled should be set to true. The allowed values are:

    NONE: Ignore any logging.
    EXECUTION: Log completion of tasks.
    PERFORMANCE: Execution + Performance logs.
    VERBOSE: All logs. Default: EXECUTION.</description>
  </property>

  <!-- Authentication/Security Configuration -->
  <property>
    <name>hive.server2.authentication</name>
    <value>NONE</value>
    <description>Client authentication types.

    NONE: no authentication check – plain SASL transport
    LDAP: LDAP/AD based authentication
    KERBEROS: Kerberos/GSSAPI authentication
    CUSTOM: Custom authentication provider (use with property hive.server2.custom.authentication.class)
    PAM: Pluggable authentication module (added in Hive 0.13.0 with HIVE-6466)
    NOSASL:  Raw transport (added in Hive 0.13.0)

    Default: NONE.</description>
  </property>

  <!-- Impersonation -->
  <property>
    <name>hive.server2.enable.doAs</name>
    <value>false</value>
    <description>Setting this property to true will have HiveServer2 execute Hive operations as the user making the calls to it. Default: true.</description>
  </property>

  <!-- Web UI -->
  <property>
    <name>hive.server2.webui.host</name>
    <value>master</value>
    <description>The host address the HiveServer2 Web UI will listen on. The Web UI can be used to access the HiveServer2 configuration, local logs, and metrics. It can also be used to check some information about active sessions and queries being executed. Default: 0.0.0.0</description>
  </property>

  <property>
    <name>hive.server2.webui.port</name>
    <value>10002</value>
    <description>The port the HiveServer2 Web UI will listen on. Set to 0 or a negative number to disable the HiveServer2 Web UI feature. Default: 10002</description>
  </property>

  <property>
    <name>hive.server2.webui.max.historic.queries</name>
    <value>25</value>
    <description>The maximum number of past queries to show in HiveServer2 Web UI. Default: 25</description>
  </property>

  <!-- HiveServer2 metrics -->
  <property>
    <name>hive.server2.metrics.enabled</name>
    <value>true</value>
    <description>Enable metrics on HiveServer2. (For other HiveServer2 configuration properties, see the HiveServer2 section.) Default: false</description>
  </property>

  <!-- For Hive 4.x
  <property>
    <name>hive.server2.webui.explain.output</name>
    <value>true</value>
    <description>The EXPLAIN EXTENDED output for the query will be shown in the WebUI,  Drilldown, Query Plan tab when this configuration property is set to true.
    Prior to Hive 3.1.0, you can use hive.log.explain.output instead of this configuration property. Default: false</description>
  </property>

  <property>
    <name>hive.server2.webui.show.graph</name>
    <value>true</value>
    <description>Set this to true to to display query plan as a graph instead of text in the WebUI. Only works with hive.server2.webui.explain.output set to true. Default: false</description>
  </property>

  <property>
    <name>hive.server2.webui.max.graph.size</name>
    <value>25</value>
    <description>Max number of stages graph can display. If number of stages exceeds this, no query plan will be shown. Only works when hive.server2.webui.show.graph and hive.server2.webui.explain.output set to true. Only works with hive.server2.webui.explain.output set to true. Default: 25</description>
  </property>

  <property>
    <name>hive.server2.webui.show.stats</name>
    <value>true</value>
    <description>Set this to true to to display statistics and log file for MapReduce tasks in the WebUI. Only works when hive.server2.webui.show.graph and hive.server2.webui.explain.output set to true. Default: false</description>
  </property>
  -->

  <!-- Query and DDL Execution -->
  <property>
    <name>hive.execution.engine</name>
    <value>tez</value>
    <description>Chooses execution engine. Options are: mr (Map Reduce, default), tez (Tez execution, for Hadoop 2 only), or spark (Spark execution, for Hive 1.1.0 onward).

    While mr remains the default engine for historical reasons, it is itself a historical engine and is deprecated in the Hive 2 line (HIVE-12300). It may be removed without further warning.

    See Hive on Tez and Hive on Spark for more information, and see the Tez section and the Spark section below for their configuration properties. Default: mr</description>
  </property>

  <property>
    <name>hive.groupby.skewindata</name>
    <value>true</value>
    <description>Whether there is skew in data to optimize group by queries. Default: false</description>
  </property>

  <property>
    <name>hive.optimize.skewjoin</name>
    <value>true</value>
    <description>Whether to enable skew join optimization.  (Also see hive.optimize.skewjoin.compiletime.) Default: false</description>
  </property>

  <property>
    <name>hive.optimize.skewjoin.compiletime</name>
    <value>true</value>
    <description>Whether to create a separate plan for skewed keys for the tables in the join. This is based on the skewed keys stored in the metadata. At compile time, the plan is broken into different joins: one for the skewed keys, and the other for the remaining keys. And then, a union is performed for the two joins generated above. So unless the same skewed key is present in both the joined tables, the join for the skewed key will be performed as a map-side join.

    The main difference between this paramater and hive.optimize.skewjoin is that this parameter uses the skew information stored in the metastore to optimize the plan at compile time itself. If there is no skew information in the metadata, this parameter will not have any effect.
    Both hive.optimize.skewjoin.compiletime and hive.optimize.skewjoin should be set to true. (Ideally, hive.optimize.skewjoin should be renamed as hive.optimize.skewjoin.runtime, but for backward compatibility that has not been done.)

    If the skew information is correctly stored in the metadata, hive.optimize.skewjoin.compiletime will change the query plan to take care of it, and hive.optimize.skewjoin will be a no-op. Default: false</description>
  </property>

  <property>
    <name>hive.optimize.union.remove</name>
    <value>true</value>
    <description>Whether to remove the union and push the operators between union and the filesink above union. This avoids an extra scan of the output by union. This is independently useful for union queries, and especially useful when hive.optimize.skewjoin.compiletime is set to true, since an extra union is inserted.

    The merge is triggered if either of hive.merge.mapfiles or hive.merge.mapredfiles is set to true. If the user has set hive.merge.mapfiles to true and hive.merge.mapredfiles to false, the idea was that the number of reducers are few, so the number of files anyway is small. However, with this optimization, we are increasing the number of files possibly by a big margin. So, we merge aggresively. Default: false</description>
  </property>

  <property>
    <name>hive.merge.mapfiles</name>
    <value>true</value>
    <description>Merge small files at the end of a map-only job. Default: true</description>
  </property>

  <property>
    <name>hive.merge.mapredfiles</name>
    <value>true</value>
    <description>Merge small files at the end of a map-reduce job. Default: false</description>
  </property>

  <property>
    <name>hive.mapred.supports.subdirectories</name>
    <value>true</value>
    <description>Whether the version of Hadoop which is running supports sub-directories for tables/partitions. Many Hive optimizations can be applied if the Hadoop version supports sub-directories for tables/partitions. This support was added by MAPREDUCE-1501. Default: false</description>
  </property>

  <property>
    <name>hive.exec.compress.output</name>
    <value>true</value>
    <description>This controls whether the final outputs of a query (to a local/hdfs file or a Hive table) is compressed. The compression codec and other options are determined from Hadoop configuration variables mapred.output.compress*. Default: false</description>
  </property>

  <property>
    <name>hive.exec.compress.intermediate</name>
    <value>true</value>
    <description>This controls whether intermediate files produced by Hive between multiple map-reduce jobs are compressed. The compression codec and other options are determined from Hadoop configuration variables mapred.output.compress*. Default: false</description>
  </property>

  <property>
    <name>hive.exec.parallel</name>
    <value>true</value>
    <description>Whether to execute jobs in parallel.  Applies to MapReduce jobs that can run in parallel, for example jobs processing different source tables before a join.  As of Hive 0.14, also applies to move tasks that can run in parallel, for example moving files to insert targets during multi-insert. Default: false</description>
  </property>

  <property>
    <name>hive.auto.convert.join</name>
    <value>true</value>
    <description>Whether Hive enables the optimization about converting common join into mapjoin based on the input file size. (Note that hive-default.xml.template incorrectly gives the default as false in Hive 0.11.0 through 0.13.1.) Default: false</description>
  </property>

  <property>
    <name>hive.optimize.correlation</name>
    <value>true</value>
    <description>Exploit intra-query correlations. For details see the Correlation Optimizer design document. Default: false</description>
  </property>

  <property>
    <name>hive.optimize.limittranspose</name>
    <value>true</value>
    <description>Whether to push a limit through left/right outer join or union. If the value is true and the size of the outer input is reduced enough (as specified in hive.optimize.limittranspose.reductionpercentage and hive.optimize.limittranspose.reductiontuples), the limit is pushed to the outer input or union; to remain semantically correct, the limit is kept on top of the join or the union too. Default: false</description>
  </property>

  <property>
    <name>hive.optimize.filter.stats.reduction</name>
    <value>true</value>
    <description>Whether to simplify comparison expressions in filter operators using column stats. Default: false</description>
  </property>

  <property>
    <name>hive.cbo.enable</name>
    <value>true</value>
    <description>When true, the cost based optimizer, which uses the Calcite framework, will be enabled. Default: false</description>
  </property>

  <property>
    <name>hive.cbo.returnpath.hiveop</name>
    <value>true</value>
    <description>When true, this optimization to CBO Logical plan will add rule to introduce not null filtering on join keys. Controls Calcite plan to Hive operator conversion. Overrides hive.optimize.remove.identity.project when set to false. Default: false</description>
  </property>

  <property>
    <name>hive.exec.dynamic.partition</name>
    <value>true</value>
    <description>Whether or not to allow dynamic partitions in DML/DDL. Default: false</description>
  </property>

  <property>
    <name>hive.limit.optimize.enable</name>
    <value>true</value>
    <description>Whether to enable to optimization to trying a smaller subset of data for simple LIMIT first. Default: false</description>
  </property>

  <property>
    <name>hive.optimize.sampling.orderby</name>
    <value>true</value>
    <description>Uses sampling on order-by clause for parallel execution. Default: false</description>
  </property>

  <property>
    <name>hive.exec.temporary.table.storage</name>
    <value>memory</value>
    <description>Expects one of [memory, ssd, default].

    Define the storage policy for temporary tables. Choices between memory, ssd and default. See HDFS Storage Types and Storage Policies. Default: default</description>
  </property>

  <!-- File format -->
  <property>
    <name>hive.default.fileformat</name>
    <value>ORC</value>
    <description>Default file format for CREATE TABLE statement. Options are TextFile, SequenceFile, RCfile, ORC, and Parquet.

    Users can explicitly say CREATE TABLE ... STORED AS TEXTFILE|SEQUENCEFILE|RCFILE|ORC|AVRO|INPUTFORMAT...OUTPUTFORMAT... to override. (RCFILE was added in Hive 0.6.0, ORC in 0.11.0, AVRO in 0.14.0, and Parquet in 2.3.0) See Row Format, Storage Format, and SerDe for details. Default: TextFile</description>
  </property>

  <property>
    <name>hive.default.fileformat.managed</name>
    <value>ORC</value>
    <description>Default file format for CREATE TABLE statement applied to managed tables only. External tables will be created with format specified by hive.default.fileformat. Options are none, TextFile, SequenceFile, RCfile, ORC, and Parquet (as of Hive 2.3.0). Leaving this null will result in using hive.default.fileformat for all native tables. For non-native tables the file format is determined by the storage handler, as shown below (see the StorageHandlers section for more information on managed/external and native/non-native terminology). Default: none</description>
  </property>

  <property>
    <name>hive.exec.orc.default.compress</name>
    <value>SNAPPY</value>
    <description>Define the default compression codec for ORC file. Default: ZLIB</description>
  </property>

  <property>
    <name>hive.orc.splits.include.file.footer</name>
    <value>true</value>
    <description>If turned on, splits generated by ORC will include metadata about the stripes in the file. This data is read remotely (from the client or HiveServer2 machine) and sent to all the tasks. Default: false</description>
  </property>

  <property>
    <name>hive.exec.orc.zerocopy</name>
    <value>true</value>
    <description>Use zerocopy reads with ORC. (This requires Hadoop 2.3 or later.). Default: false</description>
  </property>

  <!-- Vectorization -->
  <property>
    <name>hive.vectorized.execution.enabled</name>
    <value>true</value>
    <description>This flag should be set to true to enable vectorized mode of query execution. Default: false</description>
  </property>

  <property>
    <name>hive.vectorized.execution.mapjoin.minmax.enabled</name>
    <value>true</value>
    <description>This flag should be set to true to enable vector map join hash tables to use min / max filtering for integer join queries using MapJoin. Default: false</description>
  </property>

  <property>
    <name>hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled</name>
    <value>true</value>
    <description>This flag should be set to true to enable use of native fast vector map join hash tables in queries using MapJoin. Default: false</description>
  </property>

  <property>
    <name>hive.vectorized.use.vector.serde.deserialize</name>
    <value>true</value>
    <description>This flag should be set to true to enable vectorizing rows using vector deserialize. Default: false</description>
  </property>

  <!-- Scratch directory -->
  <property>
    <name>hive.exec.scratchdir</name>
    <value>hdfs://master:9000/home/hive/scratch</value>
    <description>HDFS root scratch directory for Hive jobs, which gets created with write all (733) permission. For each connecting user, an HDFS scratch directory ${hive.exec.scratchdir}/${username} is created with ${hive.scratch.dir.permission}. This directory is used by Hive to store the plans for different map/reduce stages for the query as well as to stored the intermediate outputs of these stages. Default: /tmp/hive</description>
  </property>

  <property>
    <name>hive.scratch.dir.permission</name>
    <value>700</value>
    <description>The permission for the user-specific scratch directories that get created in the root scratch directory. Default: 700</description>
  </property>
  
  <property>
    <name>hive.scratchdir.lock</name>
    <value>false</value>
    <description>When true, holds a lock file in the scratch directory. If a Hive process dies and accidentally leaves a dangling scratchdir behind, the cleardanglingscratchdir tool will remove it.

    When false, does not create a lock file and therefore the cleardanglingscratchdir tool cannot remove any dangling scratch directories. Default: false.</description>
  </property>

  <property>
    <name>hive.start.cleanup.scratchdir</name>
    <value>false</value>
    <description>To clean up the Hive scratch directory while starting the Hive server (or HiveServer2). This is not an option for a multi-user environment since it will accidentally remove the scratch directory in use. Default: false</description>
  </property>

  <!-- Index : removed since 3.0
  <property>
    <name>hive.optimize.index.filter</name>
    <value>true</value>
    <description>Whether to enable automatic use of indexes. Default: false</description>
  </property>
  -->

  <!-- Statistics -->
  <property>
    <name>hive.stats.autogather</name>
    <value>true</value>
    <description>Whether to automatically gather basic statistics during insert commands. Default: org.apache.derby.jdbc.EmbeddedDriver</description>
  </property>

  <property>
    <name>hive.stats.column.autogather</name>
    <value>true</value>
    <description>Extends statistics autogathering to also collect column level statistics. Default: false</description>
  </property>

  <property>
    <name>hive.stats.fetch.column.stats</name>
    <value>true</value>
    <description>Annotation of the operator tree with statistics information requires column statistics. Column statistics are fetched from the metastore. Fetching column statistics for each needed column can be expensive when the number of columns is high. This flag can be used to disable fetching of column statistics from the metastore. Default: false</description>
  </property>

  <property>
    <name>hive.compute.query.using.stats</name>
    <value>true</value>
    <description>When set to true Hive will answer a few queries like min, max, and count(1) purely using statistics stored in the metastore. For basic statistics collection, set the configuration property hive.stats.autogather to true. For more advanced statistics collection, run ANALYZE TABLE queries. Default: false</description>
  </property>

  <property>
    <name>hive.stats.collect.tablekeys</name>
    <value>true</value>
    <description>Whether join and group by keys on tables are derived and maintained in the QueryPlan. This is useful to identify how tables are accessed and to determine if they should be bucketed. Default: false</description>
  </property>

  <property>
    <name>hive.stats.collect.scancols</name>
    <value>true</value>
    <description>Whether column accesses are tracked in the QueryPlan. This is useful to identify how tables are accessed and to determine if there are wasted columns that can be trimmed. Default: false</description>
  </property>

  <property>
    <name>hive.stats.reliable</name>
    <value>true</value>
    <description>Whether queries will fail because statistics cannot be collected completely accurately. If this is set to true, reading/writing from/into a partition or unpartitioned table may fail because the statistics could not be computed accurately. If it is set to false, the operation will succeed.

    In Hive 0.13.0 and later, if hive.stats.reliable is false and statistics could not be computed correctly, the operation can still succeed and update the statistics but it sets a partition property "areStatsAccurate" to false. If the application needs accurate statistics, they can then be obtained in the background. Default: false</description>
  </property>

  <!-- Tez -->
  <property>
    <name>hive.tez.log.level</name>
    <value>INFO</value>
    <description>The log level to use for tasks executing as part of the DAG. Used only if hive.tez.java.opts is used to configure Java options. Default: INFO</description>
  </property>

  <property>
    <name>hive.jar.directory</name>
    <value>hdfs://master:9000/home/hive/lib</value>
    <description>This is the location that Hive in Tez mode will look for to find a site-wide installed Hive instance. See hive.user.install.directory for the default behavior. Default: (empty)</description>
  </property>

  <property>
    <name>hive.user.install.directory</name>
    <value>hdfs://master:9000/home/hive/install</value>
    <description>If Hive (in Tez mode only) cannot find a usable Hive jar in hive.jar.directory, it will upload the Hive jar to ${hive.user.install.directory}/${user_name} and use it to run queries. Default: hdfs:///user/</description>
  </property>
  
  <property>
    <name>hive.tez.container.size</name>
    <value>2048</value>
    <description>If this is not specified, the memory settings from the MapReduce configurations (mapreduce.map.memory.mb) are used by default for map tasks. Default: -1</description>
  </property>
  
  <property>
    <name>hive.tez.java.opts</name>
    <value>-XX:+UseG1GC</value>
    <description>If this is not specified, the MapReduce java opts settings (mapreduce.map.java.opts) are used by default. Default: (empty)</description>
  </property>

  <property>
    <name>hive.prewarm.enabled</name>
    <value>true</value>
    <description>Enables container prewarm for Tez (0.13.0 to 1.2.x) or Tez/Spark (1.3.0+). This is for Hadoop 2 only. Default: false</description>
  </property>

  <property>
    <name>hive.prewarm.numcontainers</name>
    <value>3</value>
    <description>Controls the number of containers to prewarm for Tez (0.13.0 to 1.2.x) or Tez/Spark (1.3.0+). This is for Hadoop 2 only. Default: 10</description>
  </property>

  <property>
    <name>hive.convert.join.bucket.mapjoin.tez</name>
    <value>true</value>
    <description>Whether joins can be automatically converted to bucket map joins in Hive when Tez is used as the execution engine (hive.execution.engine is set to "tez"). Default: false</description>
  </property>

  <property>
    <name>hive.tez.auto.reducer.parallelism</name>
    <value>true</value>
    <description>Turn on Tez' auto reducer parallelism feature. When enabled, Hive will still estimate data sizes and set parallelism estimates. Tez will sample source vertices' output sizes and adjust the estimates at runtime as necessary. Default: false</description>
  </property>

  <property>
    <name>hive.tez.exec.print.summary</name>
    <value>true</value>
    <description>If true, displays breakdown of execution steps for every query executed on Hive CLI or Beeline client. Default: false</description>
  </property>

  <property>
    <name>hive.tez.exec.inplace.progress</name>
    <value>true</value>
    <description>Updates Tez job execution progress in-place in the terminal when Hive CLI is used. Default: true</description>
  </property>

  <property>
    <name>hive.log.explain.output</name>
    <value>false</value>
    <description>When enabled, will log EXPLAIN EXTENDED output for the query at log4j INFO level and in HiveServer2 WebUI / Drilldown / Query Plan. From Hive 3.1.0 onwards, this configuration property only logs to the log4j INFO. To log the EXPLAIN EXTENDED output in WebUI / Drilldown / Query Plan from Hive 3.1.0 onwards, use hive.server2.webui.explain.output.</description>
  </property>

  <!-- Transaction -->
  <property>
    <name>hive.txn.manager</name>
    <value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
    <description>Set this to org.apache.hadoop.hive.ql.lockmgr.DbTxnManager as part of turning on Hive transactions. The default DummyTxnManager replicates pre-Hive-0.13 behavior and provides no transactions.

    Turning on Hive transactions also requires appropriate settings for hive.compactor.initiator.on, hive.compactor.worker.threads, hive.support.concurrency, hive.enforce.bucketing (Hive 0.x and 1.x only), and hive.exec.dynamic.partition.mode. Default: org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager</description>
  </property>

  <property>
    <name>hive.txn.strict.locking.mode</name>
    <value>false</value>
    <description>In strict mode non-ACID resources use standard R/W lock semantics, e.g. INSERT will acquire exclusive lock. In non-strict mode, for non-ACID resources, INSERT will only acquire shared lock, which allows two concurrent writes to the same partition but still lets lock manager prevent DROP TABLE etc. when the table is being written to.  Only apples when hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager. Default: true</description>
  </property>

  <property>
    <name>hive.exec.dynamic.partition.mode</name>
    <value>nonstrict</value>
    <description>In strict mode, the user must specify at least one static partition in case the user accidentally overwrites all partitions. In nonstrict mode all partitions are allowed to be dynamic.

    Set to nonstrict to support INSERT ... VALUES, UPDATE, and DELETE transactions (Hive 0.14.0 and later). For a complete list of parameters required for turning on Hive transactions, see hive.txn.manager. Default: strict</description>
  </property>

  <!-- Compactor -->
  <property>
    <name>hive.compactor.initiator.on</name>
    <value>true</value>
    <description>Whether to run the initiator and cleaner threads on this metastore instance. Set this to true on one instance of the Thrift metastore service as part of turning on Hive transactions. For a complete list of parameters required for turning on transactions, see hive.txn.manager.

    It's critical that this is enabled on exactly one metastore service instance (not enforced yet). Default: false</description>
  </property>

  <property>
    <name>hive.compactor.worker.threads</name>
    <value>1</value>
    <description>How many compactor worker threads to run on this metastore instance. Set this to a positive number on one or more instances of the Thrift metastore service as part of turning on Hive transactions. For a complete list of parameters required for turning on transactions, see hive.txn.manager.

    Worker threads spawn MapReduce jobs to do compactions. They do not do the compactions themselves. Increasing the number of worker threads will decrease the time it takes tables or partitions to be compacted once they are determined to need compaction. It will also increase the background load on the Hadoop cluster as more MapReduce jobs will be running in the background. Default: 0</description>
  </property>

  <property>
    <name>hive.compactor.job.queue</name>
    <value>hive</value>
  </property>

  <!-- Locking -->
  <property>
    <name>hive.support.concurrency</name>
    <value>true</value>
    <description>Whether Hive supports concurrency or not. A ZooKeeper instance must be up and running for the default Hive lock manager to support read-write locks.

    Set to true to support INSERT ... VALUES, UPDATE, and DELETE transactions (Hive 0.14.0 and later). For a complete list of parameters required for turning on Hive transactions, see hive.txn.manager. Default: false</description>
  </property>

  <property>
    <name>hive.lock.manager</name>
    <value>org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager</value>
    <description>The lock manager to use when hive.support.concurrency is set to true. Default: org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager</description>
  </property>

  <property>
    <name>hive.lock.mapred.only.operation</name>
    <value>false</value>
    <description>This configuration property is to control whether or not only do lock on queries that need to execute at least one mapred job. Default: false</description>
  </property>

  <property>
    <name>hive.zookeeper.quorum</name>
    <value>master,worker-1,worker-2,worker-3,worker-4</value>
    <description>The list of ZooKeeper servers to talk to. This is only needed for read/write locks. Default: (empty)</description>
  </property>

  <property>
    <name>hive.zookeeper.client.port</name>
    <value>2181</value>
    <description>The port of ZooKeeper servers to talk to. This is only needed for read/write locks. Default: 2181</description>
  </property>

  <property>
    <name>hive.zookeeper.namespace</name>
    <value>hive</value>
    <description>The parent node under which all ZooKeeper nodes are created. Default: hive_zookeeper_namespace</description>
  </property>

  <property>
    <name>hive.zookeeper.clean.extra.nodes</name>
    <value>true</value>
    <description>Clean extra nodes at the end of the session. Default: false</description>
  </property>

  <!-- Hive Client Security -->
  <property>
    <name>hive.security.authorization.enabled</name>
    <value>false</value>
    <description>Enable or disable the Hive client authorization. Default: false</description>
  </property>

  <!-- LLAP -->
  <property>
    <name>hive.execution.mode</name>
    <value>llap</value>
    <description>Chooses whether query fragments will run in container or in llap. Valid settings: container: launch containers; llap: utilize llap nodes during execution of tasks. Default: container</description>
  </property>

  <property>
    <name>hive.llap.execution.mode</name>
    <value>all</value>
    <description>Possible Values:
      none: not tried
      map: only map operators are considered for llap
      all: every operator is tried; but falls back to no-llap in case of problems
      only: same as "all" but stops with an exception if execution is not possible  (as of 2.2.0 with HIVE-15135) 
      auto: conversion is controlled by hive
    Chooses whether query fragments will run in a container or in LLAP. When set to "all" everything runs in LLAP if possible; "only" is like "all" but disables fallback to containers, so that the query fails if it cannot run in LLAP. Default: none</description>
  </property>

  <property>
    <name>hive.llap.io.use.lrfu</name>
    <value>true</value>
    <description>Whether ORC low-level cache should use Least Frequently/Frequently Used (LRFU) cache policy instead of default First-In-First-Out (FIFO). Default: false</description>
  </property>

  <property>
    <name>hive.llap.io.enabled</name>
    <value>true</value>
    <description>Whether the LLAP I/O layer is enabled. Remove property or set to false to disable LLAP I/O. Default: null</description>
  </property>

  <property>
    <name>hive.llap.io.memory.size</name>
    <value>1Gb</value>
    <description>Maximum size for IO allocator or ORC low-level cache. Default: 1Gb</description>
  </property>

  <property>
    <name>hive.llap.io.cache.orc.size</name>
    <value>1Gb</value>
    <description>Maximum size for IO allocator or ORC low-level cache. Default: 1Gb</description>
  </property>

  <property>
    <name>hive.llap.io.allocator.nmap</name>
    <value>true</value>
    <description>Whether ORC low-level cache should use memory mapped allocation (direct I/O). Default: false</description>
  </property>

</configuration>